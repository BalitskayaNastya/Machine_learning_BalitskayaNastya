# Методы принятия решений
Работы подготовила: Балицкая Анастсия 401-И
## Навигация

- [Метрические алгоритмы классификации](#Метрические-алгоритмы-классификации)
  - [Алгоритм 1NN и K ближайших соседей (KNN)](#алгоритм-1nn-и-k-ближайших-соседей-knn)
  - [K взвешенных ближайших соседей (kwKNN)](#k-взвешенных-ближайших-соседей-kwknn)
  
# Метрические алгоритмы классификации
**Метрические методы обучения** -- методы, основанные на анализе сходства объектов.

**_Мерой близости_** называют функцию расстояния ![](http://latex.codecogs.com/svg.latex?%5Clarge%20%5Crho%3A%20%28X%20%5Ctimes%20X%29%20%5Crightarrow%20%5Cmathbb%7BR%7D). Чем меньше расстояние между объектами, тем больше объекты похожи друг на друга.

Метрические алгоритмы классификации опираются на **_гипотезу компактности_**: схожим объектам соответствуют схожие ответы.

Метрические алгоритмы классификации с обучающей выборкой *Xl* относят объект *u* к тому классу *y*, для которого **суммарный вес ближайших обучающих объектов ![](https://latex.codecogs.com/gif.latex?W_y%28u%2C%20X%5El%29) максимален**:

![](https://latex.codecogs.com/gif.latex?W_y%28u%2C%20X%5El%29%20%3D%20%5Csum_%7Bi%20%3A%20y_%7Bu%7D%5E%7B%28i%29%7D%20%3D%20y%7D%20w%28i%2C%20u%29%20%5Crightarrow%20max)

, где весовая функция *w(i, u)* оценивает степень важности *i*-го соседа для классификации объекта *u*.

Функция ![](https://latex.codecogs.com/gif.latex?W_y%28u%2C%20X%5El%29) называется **_оценкой близости объекта u к классу y_**. Выбирая различную весовую функцию *w(i, u)* можно получать различные метрические классификаторы.

Для поиска оптимальных параметров для каждого из рассматриваемых ниже метрических алгоритмов используется **LOO -- leave-one-out** *(критерий скользящего контроля)*, который состоит в следующем: 

1. Исключать объекты *x(i)* из выборки *Xl* по одному, получится новая выборка без объекта *x(i)* (назовём её *Xl_1*).
2. Запускать алгоритм от объекта *u*, который нужно классифицировать, на выборке *Xl_1*.
3. Завести переменную *Q* (накопитель ошибки, изначально *Q = 0*) и, когда алгоритм ошибается, *Q = Q + 1*.
4. Когда все объекты *x(i)* будут перебраны, вычислить *LOO = Q / l* (*l* -- количество объектов выборки).

При минимальном значении LOO получим оптимальный параметр алгоритма.
### Алгоритм 1NN и K ближайших соседей (KNN)

**1NN:**

1. Подбирается метрика. В данной работе это декартово расстояние между векторами.
2. Обучающая выборка сортируется в порядке увеличения расстояния от классифицируемого элемента.
3. Элемент относят к тому классу к которому принадлежит ближайший (первый в отсортированной выборке) элемент.

**КNN:**

Имеется некоторая выборка *Xl*, состоящая из объектов *x(i), i = 1, ..., l* (в приложенной программе используется выборка ирисов Фишера).
Данный алгоритм классификации относит классифицируемый объект *u* к тому классу *y*, к которому относится большинство из *k* его ближайших соседей *x(u_i)*.
Для оценки близости классифицируемого объекта *u* к классу *y* **алгоритм kNN** использует следующую функцию:
![](http://latex.codecogs.com/svg.latex?%5Clarge%20W%28i%2C%20u%29%20%3D%20%5Bi%20%5Cleq%20k%5D) , где *i* -- порядок соседа по расстоянию к классифицируемому объекту *u*.

Реализация алгоритмов:

``` R
par(mfrow=c(2,1))
selected = data[c(1,3,5)]
features = dim(selected)[2]-1
cases = dim(selected)[1]
colors = c("red", "green", "blue")

dist = function(u, v) { #это возвращает эвклидовое расстояние между двумя объектами
  sqrt(sum((u-v)^2))
}

distances = function(obj, data, metric) { # это возвращает отсортированный набор данных по метрике для объекта  
dists = matrix(0, cases, 2)
  for (i in 1:cases) {
    cost = metric(obj, data[i,1:features])
    dists[i,] = c(cost, i)
  }
  idx = order(dists[,1])
  data[dists[idx,2],]
}

NN = function(obj, data, metric=dist) { # это 1-ближайший сосед  
sorted = distances(obj, data, metric)
  sorted[1,features+1]
}

kNN = function(obj, data, k, metric=dist) { # это k-ближайших соседей  
sorted = distances(obj, data, metric)
  
  n = 10 
  counts = rep(0, times=n)
  for (i in 1:k) {
    cls = sorted[i,features+1]
    counts[cls] = counts[cls] + 1
  }
  argmax = 1
  for (i in n) {
    if (counts[argmax] < counts[i]) {
      argmax = i
    }
  }
  
  cls[argmax]
}


points = rbind(#классификация 
  c(5.5, 2),
  c(6.5, 4),
  c(7, 6.5),
  c(5.4, 2.5)
)


# 1NN
plot(selected[,1], selected[,2], col=colors[selected[,features+1]], xlab="1NN", ylab="")
for (i in 1:dim(points)[1]) {
  pt = points[i,]
  points(pt[1], pt[2], col=colors[NN(pt, selected)], pch=19) 
}

# kNN
plot(selected[,1], selected[,2], col=colors[selected[,features+1]], xlab="kNN", ylab="")
for (i in 1:dim(points)[1]) {
  pt = points[i,]
  points(pt[1], pt[2], col=colors[kNN(pt, selected, 7)], pch=19) 
}
```
Вот что получилось:
![alt text](https://github.com/BalitskayaNastya/Machine_learning_BalitskayaNastya/blob/master/Метрические%20классификаторы/1_NN_kNN/результат%20рисунки.png)

Алгоритм kNN выглядит более качествеено. Для того чтобы привести более точное обоснование чем kNN лучше в этом случае, чем 1NN, следует прибегнуть к скользящему контролю.

**LOO для КNN:**

Посмотрим как отработал KNN при помощи алгоритма скользящего конторля - LOO.
![alt text](https://github.com/BalitskayaNastya/Machine_learning_BalitskayaNastya/blob/master/Метрические%20классификаторы/2_Loo_kNN/результат.png)

Минимальный LOO достигается при k=6

Програмная реализация:
``` R
getLoo = function(x) {
  l = dim(x)[1]
  n = dim(x)[2] - 1
  maxk = l
  
  loo = rep(0, times=maxk)
  
  for (i in 1:l) {
    dists = distances(x[i,], x[-i,], dist)
    for (k in 1:maxk) {
      class = applykNN(dists, k)
      if (as.integer(class) != as.integer(x[i,n+1])) {
        loo[k] = loo[k] + 1
      }
    }
    print(i)
    print(loo)
  }
  loo = loo / l
  return(loo)
}
which.min(res)
# res = getLoo(selected)
```
### Преимущества:
1. Простота реализации.
2. При *k*, подобранном около оптимального, алгоритм "неплохо" классифицирует.

### Недостатки:
1. Нужно хранить всю выборку.
2. При *k = 1* неустойчивость к погрешностям (*выбросам* -- объектам, которые окружены объектами чужого класса), вследствие чего этот выброс классифицировался неверно и окружающие его объекты, для которого он окажется ближайшим, тоже.
2. При *k = l* алгоритм наоборот чрезмерно устойчив и вырождается в константу.
3. Максимальная сумма объектов в *counts* может достигаться в нескольких классах одновременно.
4. "Скудный" набор параметров.
5. Точки, расстояние между которыми одинаково, не все будут учитываться.


### K взвешенных ближайших соседей (kwKNN)
